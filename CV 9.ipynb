{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13678532-3f11-469e-9c9d-f60235600957",
   "metadata": {},
   "source": [
    "1. What are the advantages of a CNN for image classification over a completely linked DNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09b3170-56ac-42ed-a54d-75b3c59b8be8",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks (CNNs) are specialized neural networks that are particularly well-suited for image classification tasks. Compared to a fully connected Deep Neural Network (DNN), CNNs offer several advantages for image classification, including:\n",
    "\n",
    "1. Local feature extraction: CNNs use convolutional layers that extract local features from the input image. These layers apply small filters to the input image and produce a set of feature maps that capture different patterns in the image. This makes the network more effective at detecting small, localized features in an image, which is important for tasks like object recognition.\n",
    "\n",
    "2. Parameter sharing: In a CNN, the same filter is applied to different regions of the input image, which allows the network to learn a shared set of features that are relevant across different parts of the image. This significantly reduces the number of parameters required to train the network, which makes the model more efficient and less prone to overfitting.\n",
    "\n",
    "3. Translation invariance: CNNs are also able to recognize objects regardless of their position in the image. This is because the convolutional layers use shared weights and biases, allowing them to detect the same feature regardless of where it appears in the image. This is especially important for image classification, where the position of an object in the image may vary.\n",
    "\n",
    "4. Hierarchical feature representation: The architecture of a CNN is designed to progressively learn more complex features at higher layers of the network. The early layers detect basic features like edges and corners, while later layers detect more complex features like object parts and textures. This hierarchical feature representation is important for accurate image classification, as it allows the network to learn features at multiple levels of abstraction.\n",
    "\n",
    "Overall, CNNs are highly effective at image classification because they are able to learn local, translation-invariant features from the input image, and use a hierarchical architecture to learn increasingly complex representations of those features. This makes them more efficient, accurate, and robust than fully connected DNNs for image classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34948145-d06d-48ce-b323-2d97201a075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Consider a CNN with three convolutional layers, each of which has three kernels, a stride of two,\n",
    "and SAME padding. The bottom layer generates 100 function maps, the middle layer 200, and the\n",
    "top layer 400. RGB images with a size of 200 x 300 pixels are used as input. How many criteria does\n",
    "the CNN have in total? How much RAM would this network need when making a single instance\n",
    "prediction if we're using 32-bit floats? What if you were to practice on a batch of 50 images?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fca270-6a54-4342-9fd9-98ebb4f77104",
   "metadata": {},
   "source": [
    "To calculate the total number of parameters in the CNN, we need to compute the number of parameters in each layer and sum them up. For each convolutional layer, the number of parameters is given by:\n",
    "\n",
    "(number of kernels) * (kernel size)^2 * (number of input feature maps) * (number of output feature maps) \n",
    "\n",
    "For the first convolutional layer:\n",
    "\n",
    "3 * (3^2) * 3 * 100 = 81,000 parameters\n",
    "\n",
    "For the second convolutional layer:\n",
    "\n",
    "3 * (3^2) * 100 * 200 = 1,800,000 parameters\n",
    "\n",
    "For the third convolutional layer:\n",
    "\n",
    "3 * (3^2) * 200 * 400 = 14,400,000 parameters\n",
    "\n",
    "Therefore, the total number of parameters in the CNN is:\n",
    "\n",
    "81,000 + 1,800,000 + 14,400,000 = 16,281,000 parameters\n",
    "\n",
    "To calculate the amount of RAM required for a single instance prediction, we need to compute the amount of memory required to store the activations of each layer. The amount of memory required for each layer is given by:\n",
    "\n",
    "(size of feature maps) * (number of feature maps) * (number of elements per feature map) * (size of data type)\n",
    "\n",
    "For the first convolutional layer:\n",
    "\n",
    "(100) * (100) * (150 * 100) * (4 bytes) = 600 MB\n",
    "\n",
    "For the second convolutional layer:\n",
    "\n",
    "(50) * (50) * (200 * 50) * (4 bytes) = 400 MB\n",
    "\n",
    "For the third convolutional layer:\n",
    "\n",
    "(25) * (25) * (400 * 25) * (4 bytes) = 625 MB\n",
    "\n",
    "Therefore, the total amount of RAM required for a single instance prediction is:\n",
    "\n",
    "600 MB + 400 MB + 625 MB = 1.625 GB\n",
    "\n",
    "If we were to process a batch of 50 images, the amount of memory required would be 50 times the amount required for a single instance prediction. Therefore, the total amount of RAM required for a batch of 50 images is:\n",
    "\n",
    "50 * 1.625 GB = 81.25 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5684ce38-2d95-4e6b-a699-5cd8346d26c1",
   "metadata": {},
   "source": [
    "3. What are five things you might do to fix the problem if your GPU runs out of memory while\n",
    "training a CNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62301e9e-97a5-4243-bc74-58dab33663e2",
   "metadata": {},
   "source": [
    "When training a CNN on a GPU, it is possible to encounter out of memory errors if the model is too large or if the batch size is too high. Here are five things you can try to fix the problem:\n",
    "\n",
    "1. Reduce batch size: The batch size is the number of samples processed by the network at once. If the batch size is too high, it can consume a large amount of GPU memory. You can try reducing the batch size to reduce the amount of memory used by the network.\n",
    "\n",
    "2. Use mixed precision training: Mixed precision training is a technique that uses lower-precision data types (e.g., float16 instead of float32) to reduce the memory footprint of the model. This can significantly reduce the memory requirements of the network.\n",
    "\n",
    "3. Decrease the size of the model: If the model is too large, it can consume a large amount of GPU memory. You can try reducing the size of the model by removing layers, reducing the number of filters per layer, or reducing the kernel size.\n",
    "\n",
    "4. Use gradient checkpointing: Gradient checkpointing is a technique that allows you to trade-off memory usage for compute time during backpropagation. This can help reduce the memory requirements of the network.\n",
    "\n",
    "5. Increase the size of the GPU: If the above techniques do not work, you may need to upgrade the GPU to one with more memory. A larger GPU will allow you to train larger models or use larger batch sizes without running out of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ec488a-a4b1-4503-9b3f-734bab894edd",
   "metadata": {},
   "source": [
    "4. Why would you use a max pooling layer instead with a convolutional layer of the same stride?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee1e7ec-b71e-4642-8f23-733723d7a4b6",
   "metadata": {},
   "source": [
    "Max pooling and convolutional layers serve different purposes in a convolutional neural network (CNN) despite having similar strides.\n",
    "\n",
    "A convolutional layer performs a mathematical operation known as convolution, which applies a filter/kernel to an input image to extract features. The filter/kernel slides over the image and produces a new feature map by computing the dot product of the filter/kernel weights with the input values at each position. The stride parameter determines the distance the filter/kernel moves between successive positions. \n",
    "\n",
    "On the other hand, a max pooling layer is used to downsample the feature maps produced by the convolutional layer. Max pooling reduces the dimensionality of the feature maps by partitioning each feature map into non-overlapping rectangular blocks and retaining the maximum value within each block. Max pooling has no trainable parameters and thus, does not increase the number of model parameters.\n",
    "\n",
    "Using a max pooling layer instead of a convolutional layer with the same stride can be beneficial because it can help to reduce the dimensionality of the feature maps while also introducing some form of translation invariance to small variations in the input. In other words, it allows the network to be more robust to small changes in the input images. Additionally, max pooling helps to reduce the risk of overfitting by providing a form of regularization that prevents the network from memorizing specific features of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546e401a-896f-4c6e-b052-944942594fdc",
   "metadata": {},
   "source": [
    "5. When would a local response normalization layer be useful?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c748c7-629f-4787-a3a2-f6c401fea42a",
   "metadata": {},
   "source": [
    "Local response normalization (LRN) is a technique used in deep neural networks to normalize the activity of neurons within a local neighborhood. An LRN layer is typically inserted after a convolutional layer in a neural network architecture.\n",
    "\n",
    "The purpose of an LRN layer is to encourage competition among different features computed by the same filter, as well as to promote generalization across different regions of the input. It works by normalizing the activity of a neuron relative to the activity of its neighboring neurons within a certain radius. This helps to enhance the contrast between the features computed by different filters and can improve the network's ability to generalize to new examples.\n",
    "\n",
    "An LRN layer is most useful in convolutional neural networks (CNNs) that are used for tasks such as image classification, object detection, or semantic segmentation, where the input data has a spatial structure. In these applications, an LRN layer can help to enhance the network's ability to discriminate between different features and improve its overall accuracy.\n",
    "\n",
    "However, it's worth noting that LRN has fallen out of favor in recent years and has been replaced by other normalization techniques such as batch normalization, which have been shown to be more effective in improving model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99d077c-85e5-4123-baad-cd3703ac1976",
   "metadata": {},
   "source": [
    "6. In comparison to LeNet-5, what are the main innovations in AlexNet? What about GoogLeNet and\n",
    "ResNet's core innovations?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2df5ba3-c814-4f9a-83d0-3c767a025e47",
   "metadata": {},
   "source": [
    "AlexNet, GoogLeNet, and ResNet are all famous convolutional neural network (CNN) architectures that have made significant contributions to the field of deep learning. Here are the main innovations of each architecture:\n",
    "\n",
    "1. AlexNet: AlexNet was introduced in 2012 and was the first deep CNN to win the ImageNet Large Scale Visual Recognition Challenge. The main innovations of AlexNet include:\n",
    "\n",
    "- The use of ReLU activation functions instead of traditional sigmoid functions, which greatly accelerated the convergence of the network during training.\n",
    "- The use of dropout regularization to prevent overfitting.\n",
    "- The use of data augmentation techniques such as random cropping and horizontal flipping to increase the size of the training dataset.\n",
    "\n",
    "2. GoogLeNet: GoogLeNet was introduced in 2014 and won the ImageNet challenge with a significantly lower error rate than previous winners. The main innovations of GoogLeNet include:\n",
    "\n",
    "- The use of a \"network in network\" (NiN) module, which is a small convolutional network that is used as a building block in the larger network.\n",
    "- The use of \"inception modules\", which are a combination of different convolutional filters and pooling operations that are applied in parallel to the same input data. This allows the network to capture features at different scales and resolutions.\n",
    "- The use of global average pooling instead of fully connected layers at the end of the network, which greatly reduces the number of parameters in the model and helps to prevent overfitting.\n",
    "\n",
    "3. ResNet: ResNet was introduced in 2015 and was the first CNN to use residual connections. The main innovation of ResNet is:\n",
    "\n",
    "- The use of residual connections, which allow the network to skip over some layers and learn residual functions instead of directly learning the underlying mapping. This allows the network to be much deeper than previous architectures, with up to hundreds of layers, while still maintaining good performance.\n",
    "- The use of batch normalization, which helps to stabilize the training process and speed up convergence.\n",
    "\n",
    "In comparison to LeNet-5, all three architectures are much deeper and more complex, with many more layers and parameters. They also incorporate various novel techniques such as ReLU activation functions, dropout, data augmentation, NiN modules, inception modules, global average pooling, residual connections, and batch normalization, which have significantly improved their performance on challenging computer vision tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fb49a8-94d9-462e-9f1f-ade0524e1674",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. On MNIST, build your own CNN and strive to achieve the best possible accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c987c5ec-580a-4277-8355-f2aa7ff016fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting wrapt<1.15,>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.21.8)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (21.3)\n",
      "Collecting jax>=0.3.15\n",
      "  Downloading jax-0.4.8.tar.gz (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.3.3-py2.py3-none-any.whl (26 kB)\n",
      "Collecting tensorboard<2.13,>=2.12\n",
      "  Downloading tensorboard-2.12.2-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (65.5.0)\n",
      "Collecting keras<2.13,>=2.12.0\n",
      "  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting tensorflow-estimator<2.13,>=2.12.0\n",
      "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.0-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.54.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.4)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.32.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Collecting ml-dtypes>=0.0.3\n",
      "  Downloading ml_dtypes-0.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (190 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.7 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow) (1.9.3)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.28.1)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.3.2-py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.2/242.2 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.0-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.17.3-py2.py3-none-any.whl (178 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.2/178.2 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.1)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
      "Building wheels for collected packages: jax\n",
      "  Building wheel for jax (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jax: filename=jax-0.4.8-py3-none-any.whl size=1439678 sha256=21b7ab2c5119833d501e549a3e55d06cec8a61f7ec588d84606819c012eeddb1\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/d6/f6/d5/63686989c723075de411cbc630ca12f4241a8436e411e38d6a\n",
      "Successfully built jax\n",
      "Installing collected packages: tensorboard-plugin-wit, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, opt-einsum, ml-dtypes, markdown, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, rsa, requests-oauthlib, pyasn1-modules, jax, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.0 flatbuffers-23.3.3 gast-0.4.0 google-auth-2.17.3 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.54.0 jax-0.4.8 keras-2.12.0 libclang-16.0.0 markdown-3.4.3 ml-dtypes-0.1.0 opt-einsum-3.3.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.12.2 tensorboard-data-server-0.7.0 tensorboard-plugin-wit-1.8.1 tensorflow-2.12.0 tensorflow-estimator-2.12.0 tensorflow-io-gcs-filesystem-0.32.0 termcolor-2.3.0 werkzeug-2.3.2 wrapt-1.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14550cc1-5e57-4655-8daf-6305b69327f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 19:27:49.979086: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-30 19:27:50.048684: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-30 19:27:50.050584: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-30 19:27:51.272483: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 0s 0us/step\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 20s 10ms/step - loss: 0.1338 - accuracy: 0.9591 - val_loss: 0.0583 - val_accuracy: 0.9835\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 18s 10ms/step - loss: 0.0442 - accuracy: 0.9860 - val_loss: 0.0418 - val_accuracy: 0.9853\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 18s 10ms/step - loss: 0.0309 - accuracy: 0.9903 - val_loss: 0.0334 - val_accuracy: 0.9889\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 18s 10ms/step - loss: 0.0224 - accuracy: 0.9929 - val_loss: 0.0284 - val_accuracy: 0.9908\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 18s 10ms/step - loss: 0.0160 - accuracy: 0.9949 - val_loss: 0.0280 - val_accuracy: 0.9909\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0280 - accuracy: 0.9909\n",
      "Test accuracy: 0.9908999800682068\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define the CNN model\n",
    "model = tf.keras.Sequential([\n",
    "  layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "  layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "  layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aa040b-0b0e-4e93-bec4-67fdd68d7841",
   "metadata": {},
   "source": [
    "8. Using Inception v3 to classify broad images.\n",
    "\n",
    "a.Images of different animals can be downloaded. Load them in Python using the\n",
    "matplotlib.image.mpimg.imread() or scipy.misc.imread() functions, for example. Resize and/or crop\n",
    "them to 299 x 299 pixels, and make sure they only have three channels (RGB) and no transparency.\n",
    "The photos used to train the Inception model were preprocessed to have values ranging from -1.0 to\n",
    "1.0, so make sure yours do as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1df7493-b78e-4012-be93-b66b00021e78",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/path/to/image/directory'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Load and preprocess the images\u001b[39;00m\n\u001b[1;32m     14\u001b[0m X \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     17\u001b[0m         img \u001b[38;5;241m=\u001b[39m load_img(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_dir, file), target_size\u001b[38;5;241m=\u001b[39mimg_size)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/path/to/image/directory'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "# Define the image size for Inception v3\n",
    "img_size = (299, 299)\n",
    "\n",
    "# Define the path to the directory containing the images\n",
    "image_dir = \"/path/to/image/directory\"\n",
    "\n",
    "# Load and preprocess the images\n",
    "X = []\n",
    "for file in os.listdir(image_dir):\n",
    "    if file.endswith(\".jpg\") or file.endswith(\".jpeg\") or file.endswith(\".png\"):\n",
    "        img = load_img(os.path.join(image_dir, file), target_size=img_size)\n",
    "        img_array = img_to_array(img)\n",
    "        img_array = preprocess_input(img_array)\n",
    "        X.append(img_array)\n",
    "\n",
    "# Convert the list of images to a numpy array\n",
    "X = np.array(X)\n",
    "\n",
    "# Print the shape of the input data\n",
    "print(\"Input shape:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3305e81-2e4f-456d-8aaf-4e54df38c74b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92f41d4-7bcd-4790-bb2a-1f0038aaab09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d67a650-5153-40c8-b269-9424393f233d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
