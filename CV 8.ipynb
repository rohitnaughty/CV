{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7145181-57cd-4c3f-a8cd-d595d5555101",
   "metadata": {},
   "source": [
    "1. Using our own terms and diagrams, explain INCEPTIONNET ARCHITECTURE ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424fa1cf-bff8-4be8-a505-941923475a72",
   "metadata": {},
   "source": [
    "n this architecture,there is a fixed convolution size for each layer. In the Inception module 1x1, 3x3, 5x5 convolution \n",
    "and 3x3 max pooling performed in a parallel way at the input and the output of these are stacked together to generated \n",
    "final output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebb00ce-5760-489c-a800-c1b1351aa6f8",
   "metadata": {},
   "source": [
    "2. Describe the Inception block ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0015bf38-4e3e-44cd-87f7-dd70c2c67845",
   "metadata": {},
   "source": [
    "Block that aims to approximate an optimal local sparse structure in a CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a98595b-9efc-41a7-843f-e79ce88e666b",
   "metadata": {},
   "source": [
    "3. What is the DIMENSIONALITY REDUCTION LAYER (1 LAYER CONVOLUTIONAL) ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09d8de0-4426-4516-b00d-52f62246d4fd",
   "metadata": {},
   "source": [
    "This effect of cross channel down-sampling is called \"Dimensionality reduction\". By addind 1x1 conv layer before the 5x5 \n",
    "conv, While keeeping the height and width of the feature map,we have reduced the number of operational by a factor of 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba7ed3c-6945-4f65-84d3-bbfbfafaa532",
   "metadata": {},
   "source": [
    "4. THE IMPACT OF REDUCING DIMENSIONALITY ON NETWORK PERFORMANCE ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d00a482-9fa7-4654-a8bd-4d741c5778dd",
   "metadata": {},
   "source": [
    "f the machine learning model is trained on high-dimensional data, it becomes overfitted and result in poor performance.\n",
    "Hence, it is often required to reduce the number of features,which can be done with dimentionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c605d12-72f1-4019-9c2e-6cf1b0650955",
   "metadata": {},
   "source": [
    "5. Mention three components. Style GoogLeNet ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16066c01-bb4e-47c5-ac46-5bafebd54d5f",
   "metadata": {},
   "source": [
    "This network was responsible for setting a new state-of-the-art for classification and detection in the ILSVRC.This \n",
    "first version of the Inception network is referred to as GoogleNet.\n",
    "\n",
    "The training used asynchronous stochastic gradient descent with a momentum of 0.9 and a fixed learning rate schedule\n",
    "decreasing the learning rate by 4% every 8 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7822776-0a74-4c02-8c6a-ca17445ae34b",
   "metadata": {},
   "source": [
    "6. Using our own terms and diagrams, explain RESNET ARCHITECTURE ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a063e816-1e01-414d-ac50-400224b8e87b",
   "metadata": {},
   "source": [
    "ResNet, short for Residual Networks is a classic neural network used as a backbone for many computer vision tasks.This\n",
    "model was the winner of ImageNet challenge in 2015.The fundamental breakthrough with ResNet was it allowed us to train\n",
    "expremely deep neural networks with 150+layers sucessfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bab7d15-c9ad-49b0-93b7-9b9942576c03",
   "metadata": {},
   "source": [
    "7. What do Skip Connections entail ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb36738d-269c-43ae-976b-5fbeb1149626",
   "metadata": {},
   "source": [
    "Skip Connections (or Shortcut Connections ) as the name suggests skips some of the layers in the neural network and feeds\n",
    "the output of one layer as the input to the next layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66030a04-a31b-4ead-a5f0-c58dd5717cb6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "8. What is the definition of a residual Block ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085ba8f7-092a-42d6-8027-dad0a459e6bc",
   "metadata": {},
   "source": [
    "A Residual block is a stack of layers set in such a way that the output of a layer is taken and added to another layer \n",
    "deeper in the block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903d8b3c-6969-4b81-a791-57166a0059f4",
   "metadata": {},
   "source": [
    "\n",
    "9. How can transfer learning help with problems ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8acb45-e812-46aa-be9e-cdc409ac17a3",
   "metadata": {},
   "source": [
    "Transfer learning models focus on storing knowledge gained while solving one problem and applying it to a different but \n",
    "related problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ce22fa-6e6f-4f9d-93bd-a57dfcb99f52",
   "metadata": {},
   "source": [
    "10. What is transfer learning, and how does it work ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a4449b-1682-4d28-b121-4604714a5635",
   "metadata": {},
   "source": [
    "Transfer learning is a machine learning method where we reuse a pre-trained model as the starting point for a model on a \n",
    "new task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d96cf1f-c3e5-439b-9c37-b3b7a386da17",
   "metadata": {},
   "source": [
    "11.HOW DO NEURAL NETWORKS LEARN FEATURES?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e01bcc-676f-4fc9-bb5b-d49b5680db5f",
   "metadata": {},
   "source": [
    "With convolutionl neural network, the image is fed into the network in its raw form (pixels). The network transforms\n",
    "the image many times.First, the image goes throught many convoltional layers. In those convolutional layers, the network \n",
    "learns new and increasingly complex features in its layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b84901-f3f5-4147-9606-29bd93d7e86c",
   "metadata": {},
   "source": [
    "12. WHY IS FINE-TUNING BETTER THAN START-UP TRAINING ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf78188e-0006-40c6-80d3-6327a546ec4d",
   "metadata": {},
   "source": [
    "Fine-turning is one approach to transfer learning where you change the model output to fit the new task and train only the\n",
    "output model.\n",
    "\n",
    "Assuming the original task is similar to the new task, using an artificial neural network that has already been designed \n",
    "and trained allows us to take advantage of what the model has already learning without having to develop it from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef163dd3-6013-41d7-88ba-fc49d1141919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
